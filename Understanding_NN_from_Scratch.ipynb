{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7864d671-13aa-4bf3-b362-fe9dcbcbe1b7",
   "metadata": {},
   "source": [
    "> This notebook is me trying to follow the explanations provided by Mr. Samson in his video: [Samson's Video](https://www.youtube.com/watch?v=w8yWXqWQYmU) <br>\n",
    ">Here, I'll try to make it as precise as I can. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073b48a-e7b6-45d8-8801-fd5f4c2f64dc",
   "metadata": {},
   "source": [
    "## Understanding the maths behind Neural Network by building from scratch (Just NumPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9d182-b6e7-4da0-adcf-9c334fec7c4c",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "* DataSets from [Kaggle's Digit Recognizer Dataset](https://www.kaggle.com/competitions/digit-recognizer/data)\n",
    "* It contains $28 \\times 28$ grayscale images of handwritten digits\n",
    "* Each image is accompanied by a label from **0 to 9**.\n",
    "* **Task**: Build a network that predicts what digit is written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c496221-923f-4635-9fb8-6f38400977ce",
   "metadata": {},
   "source": [
    "### Neural Network Overview\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./Notebook_images/NN_diagram_overview.png\" alt=\"NN Overview\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785f099-7dfe-4a17-ba2a-f2547aa0603b",
   "metadata": {},
   "source": [
    "#### Input Layer:\n",
    "* Input image is $28\\times28$ pixels which is equals to $784$ pixels and we insert that into out input layers. Hence, resulting in $784$nodes.\n",
    "* Here, each pixel has a value between **0 to 255**; *0 being Black* and *255 being White*.\n",
    "* Now, we need to normalize these values. So, in order to normalize, we *divide the pixels value by the max_pixel value before feeding it to the network* ie., $${Normalized\\_Value} = \\frac{{pixel\\_value}}{255}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c5789-7fb6-4f2c-8632-c978ff5ad6fd",
   "metadata": {},
   "source": [
    "#### Hidden Layer:\n",
    "* Could have any number of nodes but to make it simple, choose $10$ nodes.\n",
    "* The value of each of these nodes is calculated based on weights and biases applied to the value of the $784$ nodes in the input layer. After this calculation, a ReLU activation is applied to all nodes in the layer.\n",
    "* For simplicity just using One Hidden Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8816bc6-3b98-49a9-98b3-13a283459593",
   "metadata": {},
   "source": [
    "#### Output Layer:\n",
    "* The output layer too has $10$ nodes. **Reason: Corresponds to each digit from 0 to 9**\n",
    "* The value of each of these nodes will again be calculated from weights and biases applied to the value of the $10$ nodes in the hidden layer, with a softmax activation applied to them to get the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed02d5-6774-47c3-8926-94b57f023196",
   "metadata": {},
   "source": [
    "### Slight Notes related to Forward and Backward Propagation:\n",
    "#### Forward Prop: \n",
    "* Forward Propagation simply is a process of taking an image and running through the Neural Network to get a prediction\n",
    "* The prediction made from the given image depends on the *weights and biases* of the network.\n",
    "#### Backprop:\n",
    "* In backprop, we take previously made prediction, calculate the error of how off it was from actual value, then run this error backwards through the NN to find out how much each weight and bias parameter contributed to this error.\n",
    "* **Gradient_descent is carried out using backprop.**\n",
    "* The basic idea of gradient descent is to figure out what direction each parameter can go in to decrease error by the greatest amount, then nudge each parameter in its corresponding direction over and over again until the parameters for minimum error and highest accuracy are found. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a1ee3-2652-4d2c-8325-a262b7b5906b",
   "metadata": {},
   "source": [
    "### Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704610b0-52c4-4e57-8075-c81f13a28291",
   "metadata": {},
   "source": [
    "#### Representing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83beae58-2569-4de8-9d41-f35c1afedf3c",
   "metadata": {},
   "source": [
    "Each training example is represented by a 784-element vector, corresponding to the image's pixels. These vectors can be stacked into a matrix for vectorized calculations, allowing error computation for all examples simultaneously with matrix operations.\n",
    "\n",
    "In machine learning, it's common to stack these vectors as rows in a matrix with dimensions $m×n$, where $m$ is the number of training examples and $n$ is the number of features ($784$ in this case). To simplify calculations, we'll transpose this matrix to have dimensions $n×m$, with each column representing a training example and each row representing a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f3bf2-14f8-49e5-b36b-6f2b94dc740c",
   "metadata": {},
   "source": [
    "$$X= \\begin{bmatrix}\n",
    "x^{(1)}\\\\ x^{(2)}\\\\ .\\\\ .\\\\ x^{(m)}\\\\ \n",
    "\\end{bmatrix}^T = \\begin{bmatrix}\n",
    " x^{(1)}& x^{(2)} & . &  .&  x^{(m)}& \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94264e-f142-4403-b6df-8dfa4f6dbd9a",
   "metadata": {},
   "source": [
    "#### Representing weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c191357-6894-4193-90d2-476e5b344993",
   "metadata": {},
   "source": [
    "In a neural network, weights are represented as a matrix of dimensions $n^{[l]} \\times n^{[l-1]}$, where $n^{[l-1]}$ is the number of nodes in the previous layer and $n^{[l]}$ is the number of nodes in the current layer. For example, $W^{[1]}$ is a $10 \\times 784$ matrix, and $W^{[2]}$ is a $10 \\times 10$ matrix. Biases are constant terms added to each node of the following layer and are represented as matrices with dimensions $n^{[l]} \\times 1$, so both $b^{[1]}$ and $b^{[2]}$ have dimensions $10 \\times 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf77794-e174-492a-9a4b-778d653e5b42",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "Forward propagation in a neural network involves calculating the unactivated values of the nodes in each layer by applying weights and biases to the input.\n",
    "\n",
    "1. **First Hidden Layer**:\n",
    "   - Compute unactivated values: $Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "   - Dimensions: $X$ (784 x m), $W^{[1]}$ (10 x 784), resulting in $Z^{[1]}$ (10 x m).\n",
    "   - Bias $b^{[1]}$ (10 x 1) is broadcast to match $Z^{[1]}$.\n",
    "   - Apply activation function (ReLU): $A^{[1]} = \\text{ReLU}(Z^{[1]})$.\n",
    "\n",
    "2. **Second Layer (Output Layer)**:\n",
    "   - Compute unactivated values: $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$.\n",
    "   - Apply activation function (softmax): $A^{[2]} = \\text{softmax}(Z^{[2]})$.\n",
    "\n",
    "   - Dimensions: $Z^{[2]}$ and $A^{[2]}$ are both (10 x m).\n",
    "\n",
    "The softmax function outputs probabilities for each class, allowing the network to predict the likelihood that a given input belongs to each class. The final output matrix $A^{[2]}$ provides these prediction probabilities for all training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52c674-5ce4-4f39-9d48-b739202eacda",
   "metadata": {},
   "source": [
    "#### Backward Propagation\n",
    "Backward propagation involves computing how to adjust the neural network's parameters to minimize the loss function. For a softmax classifier, we use a cross-entropy loss function, defined as:\n",
    "\n",
    "$$J(\\hat{y}, y) = -\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Here, $\\hat{y}$ is our prediction vector, and $y$ is the one-hot encoded correct label. The loss for a given example is the log of the probability assigned to the correct prediction. The goal is to minimize this loss by updating the parameters using gradient descent.\n",
    "\n",
    "We compute the derivative of the loss function with respect to each parameter. For simplicity, these derivatives are denoted as $dW^{[1]}$, $db^{[1]}$, $dW^{[2]}$, and $db^{[2]}$. The process starts by calculating $dA^{[2]}$, the derivative of the loss with respect to the output of the second layer:\n",
    "\n",
    "$$dA^{[2]} = Y - A^{[2]}$$\n",
    "\n",
    "From $dA^{[2]}$, we calculate $dW^{[2]}$ and $db^{[2]}$ as follows:\n",
    "\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "\n",
    "$$db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}$$\n",
    "\n",
    "Next, to find $dW^{[1]}$ and $db^{[1]}$, we first determine $dZ^{[1]}$:\n",
    "\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} \\cdot g^{[1]\\prime}(Z^{[1]})$$\n",
    "\n",
    "Since our activation function is ReLU, its derivative is 1 for positive input values and 0 for negative ones. Thus, $g^{[1]\\prime}(Z^{[1]})$ is a matrix of 1s and 0s based on the values of $Z^{[1]}$.\n",
    "\n",
    "Finally, we calculate $dW^{[1]}$ and $db^{[1]}$:\n",
    "\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T$$\n",
    "\n",
    "$$db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}$$\n",
    "\n",
    "Once we have all the derivatives, we update our parameters:\n",
    "\n",
    "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "\n",
    "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
    "\n",
    "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "\n",
    "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n",
    "\n",
    "Here, $\\alpha$ is the learning rate, a hyperparameter that controls how much we adjust the parameters in each iteration.\n",
    "\n",
    "To summarize the process: we first perform forward propagation to compute the predictions:\n",
    "\n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "\n",
    "$$A^{[1]} = \\text{ReLU}(Z^{[1]})$$\n",
    "\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
    "\n",
    "$$A^{[2]} = \\text{softmax}(Z^{[2]})$$\n",
    "\n",
    "Then, we perform backpropagation to compute the gradients:\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y$$\n",
    "\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "\n",
    "$$db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}$$\n",
    "\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} \\cdot g^{[1]\\prime}(Z^{[1]})$$\n",
    "\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T$$\n",
    "\n",
    "$$db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}$$\n",
    "\n",
    "Finally, we update the parameters:\n",
    "\n",
    "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "\n",
    "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
    "\n",
    "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "\n",
    "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n",
    "\n",
    "This process is repeated iteratively until the model's performance is satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b208b7-cb50-472d-8a6a-71f6071b2233",
   "metadata": {},
   "source": [
    "### The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5f9cc-e551-47f1-b8da-208694eac943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Session for tommorow 😊"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
