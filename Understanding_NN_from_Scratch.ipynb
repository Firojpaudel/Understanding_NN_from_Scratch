{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7864d671-13aa-4bf3-b362-fe9dcbcbe1b7",
   "metadata": {},
   "source": [
    "> This notebook is me trying to follow the explanations provided by Mr. Samson in his video: [Samson's Video](https://www.youtube.com/watch?v=w8yWXqWQYmU) <br>\n",
    ">Here, I'll try to make it as precise as I can. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073b48a-e7b6-45d8-8801-fd5f4c2f64dc",
   "metadata": {},
   "source": [
    "## Understanding the maths behind Neural Network by building from scratch (Just NumPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9d182-b6e7-4da0-adcf-9c334fec7c4c",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "* DataSets from [Kaggle's Digit Recognizer Dataset](https://www.kaggle.com/competitions/digit-recognizer/data)\n",
    "* It contains $28 \\times 28$ grayscale images of handwritten digits\n",
    "* Each image is accompanied by a label from **0 to 9**.\n",
    "* **Task**: Build a network that predicts what digit is written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c496221-923f-4635-9fb8-6f38400977ce",
   "metadata": {},
   "source": [
    "### Neural Network Overview\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./Notebook_images/NN_diagram_overview.png\" alt=\"NN Overview\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785f099-7dfe-4a17-ba2a-f2547aa0603b",
   "metadata": {},
   "source": [
    "#### Input Layer:\n",
    "* Input image is $28\\times28$ pixels which is equals to $784$ pixels and we insert that into out input layers. Hence, resulting in $784$nodes.\n",
    "* Here, each pixel has a value between **0 to 255**; *0 being Black* and *255 being White*.\n",
    "* Now, we need to normalize these values. So, in order to normalize, we *divide the pixels value by the max_pixel value before feeding it to the network* ie., $${Normalized\\_Value} = \\frac{{pixel\\_value}}{255}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c5789-7fb6-4f2c-8632-c978ff5ad6fd",
   "metadata": {},
   "source": [
    "#### Hidden Layer:\n",
    "* Could have any number of nodes but to make it simple, choose $10$ nodes.\n",
    "* The value of each of these nodes is calculated based on weights and biases applied to the value of the $784$ nodes in the input layer. After this calculation, a ReLU activation is applied to all nodes in the layer.\n",
    "* For simplicity just using One Hidden Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8816bc6-3b98-49a9-98b3-13a283459593",
   "metadata": {},
   "source": [
    "#### Output Layer:\n",
    "* The output layer too has $10$ nodes. **Reason: Corresponds to each digit from 0 to 9**\n",
    "* The value of each of these nodes will again be calculated from weights and biases applied to the value of the $10$ nodes in the hidden layer, with a softmax activation applied to them to get the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed02d5-6774-47c3-8926-94b57f023196",
   "metadata": {},
   "source": [
    "### Slight Notes related to Forward and Backward Propagation:\n",
    "#### Forward Prop: \n",
    "* Forward Propagation simply is a process of taking an image and running through the Neural Network to get a prediction\n",
    "* The prediction made from the given image depends on the *weights and biases* of the network.\n",
    "#### Backprop:\n",
    "* In backprop, we take previously made prediction, calculate the error of how off it was from actual value, then run this error backwards through the NN to find out how much each weight and bias parameter contributed to this error.\n",
    "* **Gradient_descent is carried out using backprop.**\n",
    "* The basic idea of gradient descent is to figure out what direction each parameter can go in to decrease error by the greatest amount, then nudge each parameter in its corresponding direction over and over again until the parameters for minimum error and highest accuracy are found. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a1ee3-2652-4d2c-8325-a262b7b5906b",
   "metadata": {},
   "source": [
    "### Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704610b0-52c4-4e57-8075-c81f13a28291",
   "metadata": {},
   "source": [
    "#### Representing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83beae58-2569-4de8-9d41-f35c1afedf3c",
   "metadata": {},
   "source": [
    "Each training example is represented by a 784-element vector, corresponding to the image's pixels. These vectors can be stacked into a matrix for vectorized calculations, allowing error computation for all examples simultaneously with matrix operations.\n",
    "\n",
    "In machine learning, it's common to stack these vectors as rows in a matrix with dimensions $m×n$, where $m$ is the number of training examples and $n$ is the number of features ($784$ in this case). To simplify calculations, we'll transpose this matrix to have dimensions $n×m$, with each column representing a training example and each row representing a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f3bf2-14f8-49e5-b36b-6f2b94dc740c",
   "metadata": {},
   "source": [
    "$$X= \\begin{bmatrix}\n",
    "x^{(1)}\\\\ x^{(2)}\\\\ .\\\\ .\\\\ x^{(m)}\\\\ \n",
    "\\end{bmatrix}^T = \\begin{bmatrix}\n",
    " x^{(1)}& x^{(2)} & . &  .&  x^{(m)}& \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94264e-f142-4403-b6df-8dfa4f6dbd9a",
   "metadata": {},
   "source": [
    "#### Representing weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c191357-6894-4193-90d2-476e5b344993",
   "metadata": {},
   "source": [
    "In a neural network, weights are represented as a matrix of dimensions $n^{[l]} \\times n^{[l-1]}$, where $n^{[l-1]}$ is the number of nodes in the previous layer and $n^{[l]}$ is the number of nodes in the current layer. For example, $W^{[1]}$ is a $10 \\times 784$ matrix, and $W^{[2]}$ is a $10 \\times 10$ matrix. Biases are constant terms added to each node of the following layer and are represented as matrices with dimensions $n^{[l]} \\times 1$, so both $b^{[1]}$ and $b^{[2]}$ have dimensions $10 \\times 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf77794-e174-492a-9a4b-778d653e5b42",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "Forward propagation in a neural network involves calculating the unactivated values of the nodes in each layer by applying weights and biases to the input.\n",
    "\n",
    "1. **First Hidden Layer**:\n",
    "   - Compute unactivated values: $Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "   - Dimensions: $X$ (784 x m), $W^{[1]}$ (10 x 784), resulting in $Z^{[1]}$ (10 x m).\n",
    "   - Bias $b^{[1]}$ (10 x 1) is broadcast to match $Z^{[1]}$.\n",
    "   - Apply activation function (ReLU): $A^{[1]} = \\text{ReLU}(Z^{[1]})$.\n",
    "\n",
    "2. **Second Layer (Output Layer)**:\n",
    "   - Compute unactivated values: $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$.\n",
    "   - Apply activation function (softmax): $A^{[2]} = \\text{softmax}(Z^{[2]})$.\n",
    "\n",
    "   - Dimensions: $Z^{[2]}$ and $A^{[2]}$ are both (10 x m).\n",
    "\n",
    "The softmax function outputs probabilities for each class, allowing the network to predict the likelihood that a given input belongs to each class. The final output matrix $A^{[2]}$ provides these prediction probabilities for all training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52c674-5ce4-4f39-9d48-b739202eacda",
   "metadata": {},
   "source": [
    "#### Backward Propagation\n",
    "Backward propagation involves computing how to adjust the neural network's parameters to minimize the loss function. For a softmax classifier, we use a cross-entropy loss function, defined as:\n",
    "\n",
    "$$J(\\hat{y}, y) = -\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Here, $\\hat{y}$ is our prediction vector, and $y$ is the one-hot encoded correct label. The loss for a given example is the log of the probability assigned to the correct prediction. The goal is to minimize this loss by updating the parameters using gradient descent.\n",
    "\n",
    "We compute the derivative of the loss function with respect to each parameter. For simplicity, these derivatives are denoted as $dW^{[1]}$, $db^{[1]}$, $dW^{[2]}$, and $db^{[2]}$. The process starts by calculating $dA^{[2]}$, the derivative of the loss with respect to the output of the second layer:\n",
    "\n",
    "$$dA^{[2]} = Y - A^{[2]}$$\n",
    "\n",
    "From $dA^{[2]}$, we calculate $dW^{[2]}$ and $db^{[2]}$ as follows:\n",
    "\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "\n",
    "$$db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}$$\n",
    "\n",
    "Next, to find $dW^{[1]}$ and $db^{[1]}$, we first determine $dZ^{[1]}$:\n",
    "\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} \\cdot g^{[1]\\prime}(Z^{[1]})$$\n",
    "\n",
    "Since our activation function is ReLU, its derivative is 1 for positive input values and 0 for negative ones. Thus, $g^{[1]\\prime}(Z^{[1]})$ is a matrix of 1s and 0s based on the values of $Z^{[1]}$.\n",
    "\n",
    "Finally, we calculate $dW^{[1]}$ and $db^{[1]}$:\n",
    "\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T$$\n",
    "\n",
    "$$db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}$$\n",
    "\n",
    "Once we have all the derivatives, we update our parameters:\n",
    "\n",
    "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "\n",
    "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
    "\n",
    "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "\n",
    "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n",
    "\n",
    "Here, $\\alpha$ is the learning rate, a hyperparameter that controls how much we adjust the parameters in each iteration.\n",
    "\n",
    "To summarize the process: we first perform forward propagation to compute the predictions:\n",
    "\n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "\n",
    "$$A^{[1]} = \\text{ReLU}(Z^{[1]})$$\n",
    "\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
    "\n",
    "$$A^{[2]} = \\text{softmax}(Z^{[2]})$$\n",
    "\n",
    "Then, we perform backpropagation to compute the gradients:\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y$$\n",
    "\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "\n",
    "$$db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}$$\n",
    "\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} \\cdot g^{[1]\\prime}(Z^{[1]})$$\n",
    "\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T$$\n",
    "\n",
    "$$db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}$$\n",
    "\n",
    "Finally, we update the parameters:\n",
    "\n",
    "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "\n",
    "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
    "\n",
    "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "\n",
    "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n",
    "\n",
    "This process is repeated iteratively until the model's performance is satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b208b7-cb50-472d-8a6a-71f6071b2233",
   "metadata": {},
   "source": [
    "### The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de5f9cc-e551-47f1-b8da-208694eac943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966e029-718d-4bd2-9a23-6461d559d378",
   "metadata": {},
   "source": [
    "Use cases explained: \n",
    "* Numpy for linear algebra :: working with matrices\n",
    "* Pandas for reading the data\n",
    "* Pyplot for displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9ad870-ed96-45e3-92d1-6d34d8b37c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data:\n",
    "data = pd.read_csv('./digit_recognizer_dataset/train.csv')\n",
    "#Now converting the pandas data to Numpy as we are solely working on Numpy \n",
    "data= np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8104cfe1-f0c9-451f-9740-7f4d6ee8cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n = data.shape #m= 42000 and n = 785\n",
    "# Now shuffling the dataset before splitting it into test-set and cross-validation set\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1203f-5286-4cc3-be5f-7ee5da3388b5",
   "metadata": {},
   "source": [
    "#### Preparing data for Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32839574-9ce6-4d83-8bb2-e2fdc7591a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# development set preparation aka. Cross validation Set \n",
    "data_dev = data[0:1000].T\n",
    "# Reason behind transposing:: As mentioned previously above, it's used for simplifying calculations.\n",
    "Y_dev= data_dev[0]\n",
    "#Extracts the labels (first row) for the development set.\n",
    "X_dev= data_dev[1:n]\n",
    "#Extracting the features (remaining rows) fo the development set.\n",
    "\n",
    "#And now normalizing the pixels as mentioned previously (255 is the maximum possible value for a pixel)\n",
    "X_dev = X_dev/ 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab97928-ef88-4542-a729-05dbc870c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Set Preparation \n",
    "data_train = data[1000:m].T\n",
    "\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n] #Similar as above no need for further explanation\n",
    "\n",
    "X_train = X_train/255\n",
    "\n",
    "# #Training set shape \n",
    "# X_train.shape    #Output: (784, 41000)\n",
    "\n",
    "#Now, Since the feature number is not required, we just neglect it and take the number of training examples by just feeding the values \n",
    "_, m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4be3d08-c157-4445-90dc-515add427955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 1, 4, ..., 9, 6, 2], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking up at the labels of our training set \n",
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b2e2c-8580-4bf0-b327-66cfa46b64b9",
   "metadata": {},
   "source": [
    "#### Now time for the actual Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c5cf1b8-5e72-486e-900a-2a03a50836bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First defining the initial parameters for the NN (both layers)\n",
    "\n",
    "def init_param():\n",
    "    W1 = np.random.rand(10, 784) -0.5 \n",
    "    '''\n",
    "    Explaining the code snippet above:\n",
    "    np.random.rand(a,b) function creates an array of 2D with random values from uniform distribution over [0,1) \n",
    "    then substracting 0.5 from each element which shifts the distribution to [-0.5 , 0.5) \n",
    "\n",
    "    Subtracting 0.5 from the randomly initialized weights ensures that:\n",
    "    - The neurons start with different weights (breaking symmetry).\n",
    "    - The initial activations are centered around zero, promoting balanced gradient flow.\n",
    "    - This initialization helps in avoiding saturation of activation functions and improves the efficiency of the learning process.\n",
    "    \n",
    "    '''\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b0d8f57-917f-4143-b6af-96db2de14ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now defining the activation functions:\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def softmax(Z):\n",
    "    S = np.exp(Z)/ sum(np.exp(Z))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88add2d-4e1c-4f50-ad64-d0a5fa2db4ba",
   "metadata": {},
   "source": [
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "and $$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82cbf5b-4cdd-4705-88b5-40a778b10d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for forward_propagation :: the above equations in forward prop is implemented in this block...\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1   # Z = Wx + b \n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# For calculating the derivative of loss function dZ we need to calculate the derivative of activation function which here is ReLU. \n",
    "# So, derivative of ReLU function is defined as:\n",
    "\n",
    "def derivative_ReLU(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc669fc-5835-4798-9bb6-47c8df2e4a96",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{ReLU}'(Z) = \\begin{cases}\n",
    "0 & \\text{if } Z \\leq 0 \\\\\n",
    "1 & \\text{if } Z > 0\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd82199d-651c-4356-907d-7e2f738d8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then one-hot encoding y ie. our labels \n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1)) #Cretes a matrix of zeros where rows:: sizeof Y and columns:: maxof Y + 1 \n",
    "    '''\n",
    "        Here, max of Y would be 9 since arrays initialize from 0 and taking 9 as number of columns while one-hot encoding would do no good so \n",
    "        in order to place all 10 labels from 0 to 9 in columns +1 was necessary\n",
    "    '''\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    '''\n",
    "        This is similar to setting Y[i] = 1 for each row i. But a numpy approach\n",
    "    '''\n",
    "    one_hot_Y = one_hot_Y.T # Now the one-hot encoded vectors are not column vectors\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce46167-201f-49e5-972e-90caba771fac",
   "metadata": {},
   "source": [
    "#### Pictorial representation of One-hot Encoding\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./Notebook_images/One_hot.png\" alt=\"One_hot.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "139f8863-e610-446d-b29b-7644472512b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After One-hot encoding is completed going for backprop\n",
    "def back_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y # The Y in above equations was one-hot encoded ... \n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * derivative_ReLU(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "562644bd-63a5-4057-b5e2-de142536e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then updating the parameters: \n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bdacd5-d0ac-42ff-9ac3-cdba3fe529ac",
   "metadata": {},
   "source": [
    "#### Now Time For Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46fe6dda-1d2a-4672-b488-002d397cc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "    \n",
    "def gradient_descent(X, Y, alpha, iter):\n",
    "    W1, b1, W2, b2 = init_param() #inheriting the variables  \n",
    "    for i in range(iter):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "            '''\n",
    "                Note: It's not necessarily for the numbers of iterations to be divisible by 10. It's just a way to show progress every 10 iterations\n",
    "                in the output console... \n",
    "            '''\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f5fd6bb-02dc-45ec-ab8a-beea1b95c10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[2 2 2 ... 2 2 2] [9 1 4 ... 9 6 2]\n",
      "0.10317073170731707\n",
      "Iteration:  10\n",
      "[2 5 2 ... 2 1 0] [9 1 4 ... 9 6 2]\n",
      "0.165\n",
      "Iteration:  20\n",
      "[2 1 2 ... 2 1 0] [9 1 4 ... 9 6 2]\n",
      "0.25129268292682927\n",
      "Iteration:  30\n",
      "[1 1 2 ... 0 1 0] [9 1 4 ... 9 6 2]\n",
      "0.3114390243902439\n",
      "Iteration:  40\n",
      "[1 1 0 ... 0 1 9] [9 1 4 ... 9 6 2]\n",
      "0.36429268292682926\n",
      "Iteration:  50\n",
      "[1 1 0 ... 0 1 9] [9 1 4 ... 9 6 2]\n",
      "0.40968292682926827\n",
      "Iteration:  60\n",
      "[1 1 4 ... 7 6 9] [9 1 4 ... 9 6 2]\n",
      "0.45214634146341465\n",
      "Iteration:  70\n",
      "[1 1 4 ... 7 6 5] [9 1 4 ... 9 6 2]\n",
      "0.48821951219512194\n",
      "Iteration:  80\n",
      "[2 1 4 ... 7 6 5] [9 1 4 ... 9 6 2]\n",
      "0.5179024390243903\n",
      "Iteration:  90\n",
      "[2 1 4 ... 7 6 5] [9 1 4 ... 9 6 2]\n",
      "0.5457804878048781\n",
      "Iteration:  100\n",
      "[8 1 4 ... 7 6 5] [9 1 4 ... 9 6 2]\n",
      "0.5699268292682926\n",
      "Iteration:  110\n",
      "[8 1 4 ... 7 6 5] [9 1 4 ... 9 6 2]\n",
      "0.5923414634146341\n",
      "Iteration:  120\n",
      "[8 1 4 ... 7 6 5] [9 1 4 ... 9 6 2]\n",
      "0.613\n",
      "Iteration:  130\n",
      "[8 1 4 ... 7 6 5] [9 1 4 ... 9 6 2]\n",
      "0.6334878048780488\n",
      "Iteration:  140\n",
      "[8 1 4 ... 7 2 5] [9 1 4 ... 9 6 2]\n",
      "0.6524878048780488\n",
      "Iteration:  150\n",
      "[8 1 4 ... 7 2 5] [9 1 4 ... 9 6 2]\n",
      "0.6695609756097561\n",
      "Iteration:  160\n",
      "[8 1 4 ... 7 2 8] [9 1 4 ... 9 6 2]\n",
      "0.6847317073170732\n",
      "Iteration:  170\n",
      "[8 1 4 ... 7 2 8] [9 1 4 ... 9 6 2]\n",
      "0.6972439024390243\n",
      "Iteration:  180\n",
      "[8 1 4 ... 7 2 8] [9 1 4 ... 9 6 2]\n",
      "0.710170731707317\n",
      "Iteration:  190\n",
      "[8 1 4 ... 7 2 8] [9 1 4 ... 9 6 2]\n",
      "0.7225365853658536\n",
      "Iteration:  200\n",
      "[8 1 4 ... 7 6 8] [9 1 4 ... 9 6 2]\n",
      "0.7329756097560975\n",
      "Iteration:  210\n",
      "[8 1 4 ... 7 6 8] [9 1 4 ... 9 6 2]\n",
      "0.7418536585365854\n",
      "Iteration:  220\n",
      "[8 1 4 ... 7 6 8] [9 1 4 ... 9 6 2]\n",
      "0.7500243902439024\n",
      "Iteration:  230\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.7581707317073171\n",
      "Iteration:  240\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.7651219512195122\n",
      "Iteration:  250\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.7708780487804878\n",
      "Iteration:  260\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.7769024390243903\n",
      "Iteration:  270\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.7831219512195122\n",
      "Iteration:  280\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.7883414634146342\n",
      "Iteration:  290\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.7924634146341464\n",
      "Iteration:  300\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.796829268292683\n",
      "Iteration:  310\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8008780487804879\n",
      "Iteration:  320\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8043414634146342\n",
      "Iteration:  330\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8079756097560976\n",
      "Iteration:  340\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8114878048780488\n",
      "Iteration:  350\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.814560975609756\n",
      "Iteration:  360\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.817560975609756\n",
      "Iteration:  370\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8199512195121951\n",
      "Iteration:  380\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8225365853658536\n",
      "Iteration:  390\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8256097560975609\n",
      "Iteration:  400\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8275121951219512\n",
      "Iteration:  410\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8295853658536585\n",
      "Iteration:  420\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8312926829268292\n",
      "Iteration:  430\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8330975609756097\n",
      "Iteration:  440\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.834780487804878\n",
      "Iteration:  450\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8366829268292683\n",
      "Iteration:  460\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8380243902439024\n",
      "Iteration:  470\n",
      "[8 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8395853658536585\n",
      "Iteration:  480\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.841170731707317\n",
      "Iteration:  490\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.842609756097561\n",
      "Iteration:  500\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8442682926829268\n",
      "Iteration:  510\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8456341463414634\n",
      "Iteration:  520\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8470487804878049\n",
      "Iteration:  530\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8479268292682927\n",
      "Iteration:  540\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8486829268292683\n",
      "Iteration:  550\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8498780487804878\n",
      "Iteration:  560\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8510487804878049\n",
      "Iteration:  570\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8521951219512195\n",
      "Iteration:  580\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8530243902439024\n",
      "Iteration:  590\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8539024390243902\n",
      "Iteration:  600\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8548292682926829\n",
      "Iteration:  610\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8557560975609756\n",
      "Iteration:  620\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8567073170731707\n",
      "Iteration:  630\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8575365853658536\n",
      "Iteration:  640\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8581463414634146\n",
      "Iteration:  650\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8590975609756097\n",
      "Iteration:  660\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8598292682926829\n",
      "Iteration:  670\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8604146341463415\n",
      "Iteration:  680\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8610487804878049\n",
      "Iteration:  690\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8616585365853658\n",
      "Iteration:  700\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8623170731707317\n",
      "Iteration:  710\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8628292682926829\n",
      "Iteration:  720\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8634146341463415\n",
      "Iteration:  730\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8638780487804878\n",
      "Iteration:  740\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8643414634146341\n",
      "Iteration:  750\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8647560975609756\n",
      "Iteration:  760\n",
      "[9 1 4 ... 7 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8652439024390244\n",
      "Iteration:  770\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.865780487804878\n",
      "Iteration:  780\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8665365853658537\n",
      "Iteration:  790\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8669268292682927\n",
      "Iteration:  800\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8675365853658537\n",
      "Iteration:  810\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8679024390243902\n",
      "Iteration:  820\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8683170731707317\n",
      "Iteration:  830\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8688780487804878\n",
      "Iteration:  840\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8692682926829268\n",
      "Iteration:  850\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8697560975609756\n",
      "Iteration:  860\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8701951219512195\n",
      "Iteration:  870\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.870609756097561\n",
      "Iteration:  880\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8708536585365854\n",
      "Iteration:  890\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8710731707317073\n",
      "Iteration:  900\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8715121951219512\n",
      "Iteration:  910\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8719512195121951\n",
      "Iteration:  920\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8721219512195122\n",
      "Iteration:  930\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8725609756097561\n",
      "Iteration:  940\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8729024390243902\n",
      "Iteration:  950\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8731951219512195\n",
      "Iteration:  960\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8735121951219512\n",
      "Iteration:  970\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8739512195121951\n",
      "Iteration:  980\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8743658536585366\n",
      "Iteration:  990\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8747560975609756\n",
      "Iteration:  1000\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8751219512195122\n",
      "Iteration:  1010\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8755609756097561\n",
      "Iteration:  1020\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8758536585365854\n",
      "Iteration:  1030\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.876\n",
      "Iteration:  1040\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8762439024390244\n",
      "Iteration:  1050\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8765365853658537\n",
      "Iteration:  1060\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8768536585365854\n",
      "Iteration:  1070\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8772682926829268\n",
      "Iteration:  1080\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8775853658536585\n",
      "Iteration:  1090\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8781951219512195\n",
      "Iteration:  1100\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8783658536585366\n",
      "Iteration:  1110\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8787073170731707\n",
      "Iteration:  1120\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.879\n",
      "Iteration:  1130\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8792926829268293\n",
      "Iteration:  1140\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8796585365853659\n",
      "Iteration:  1150\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8799512195121951\n",
      "Iteration:  1160\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.880219512195122\n",
      "Iteration:  1170\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8805365853658537\n",
      "Iteration:  1180\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8807317073170732\n",
      "Iteration:  1190\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8809512195121951\n",
      "Iteration:  1200\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8812439024390244\n",
      "Iteration:  1210\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8817317073170732\n",
      "Iteration:  1220\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8821219512195122\n",
      "Iteration:  1230\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8826341463414634\n",
      "Iteration:  1240\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8829512195121951\n",
      "Iteration:  1250\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8832439024390244\n",
      "Iteration:  1260\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8833414634146342\n",
      "Iteration:  1270\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8835609756097561\n",
      "Iteration:  1280\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8839024390243903\n",
      "Iteration:  1290\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8842682926829268\n",
      "Iteration:  1300\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8846341463414634\n",
      "Iteration:  1310\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8848536585365854\n",
      "Iteration:  1320\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8852682926829268\n",
      "Iteration:  1330\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8855365853658537\n",
      "Iteration:  1340\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8859268292682927\n",
      "Iteration:  1350\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8862439024390244\n",
      "Iteration:  1360\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8862926829268293\n",
      "Iteration:  1370\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8865853658536585\n",
      "Iteration:  1380\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8867560975609756\n",
      "Iteration:  1390\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8869024390243903\n",
      "Iteration:  1400\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8870975609756098\n",
      "Iteration:  1410\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8874146341463415\n",
      "Iteration:  1420\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8877073170731707\n",
      "Iteration:  1430\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8880731707317073\n",
      "Iteration:  1440\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8881707317073171\n",
      "Iteration:  1450\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8884390243902439\n",
      "Iteration:  1460\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8887073170731707\n",
      "Iteration:  1470\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8890487804878049\n",
      "Iteration:  1480\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8893414634146342\n",
      "Iteration:  1490\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8893658536585366\n",
      "Iteration:  1500\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8894634146341464\n",
      "Iteration:  1510\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8899512195121951\n",
      "Iteration:  1520\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.890219512195122\n",
      "Iteration:  1530\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8903658536585366\n",
      "Iteration:  1540\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8905121951219512\n",
      "Iteration:  1550\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8905365853658537\n",
      "Iteration:  1560\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8907560975609756\n",
      "Iteration:  1570\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8910731707317073\n",
      "Iteration:  1580\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8913658536585366\n",
      "Iteration:  1590\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8914878048780488\n",
      "Iteration:  1600\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8914878048780488\n",
      "Iteration:  1610\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8916341463414634\n",
      "Iteration:  1620\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.892\n",
      "Iteration:  1630\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8922926829268293\n",
      "Iteration:  1640\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8924878048780488\n",
      "Iteration:  1650\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8926341463414634\n",
      "Iteration:  1660\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8928292682926829\n",
      "Iteration:  1670\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8930243902439025\n",
      "Iteration:  1680\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8931707317073171\n",
      "Iteration:  1690\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8933170731707317\n",
      "Iteration:  1700\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8935365853658537\n",
      "Iteration:  1710\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8936341463414634\n",
      "Iteration:  1720\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8937317073170732\n",
      "Iteration:  1730\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8938292682926829\n",
      "Iteration:  1740\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.894\n",
      "Iteration:  1750\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8941951219512195\n",
      "Iteration:  1760\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.894390243902439\n",
      "Iteration:  1770\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8948048780487805\n",
      "Iteration:  1780\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8947804878048781\n",
      "Iteration:  1790\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8948536585365854\n",
      "Iteration:  1800\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8951219512195122\n",
      "Iteration:  1810\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8952926829268293\n",
      "Iteration:  1820\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8954146341463415\n",
      "Iteration:  1830\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8955853658536586\n",
      "Iteration:  1840\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8957073170731708\n",
      "Iteration:  1850\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8959268292682927\n",
      "Iteration:  1860\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8961219512195122\n",
      "Iteration:  1870\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8962439024390244\n",
      "Iteration:  1880\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8964878048780488\n",
      "Iteration:  1890\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.896609756097561\n",
      "Iteration:  1900\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8966829268292683\n",
      "Iteration:  1910\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8967804878048781\n",
      "Iteration:  1920\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8969024390243903\n",
      "Iteration:  1930\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8970731707317073\n",
      "Iteration:  1940\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8973414634146342\n",
      "Iteration:  1950\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8975365853658537\n",
      "Iteration:  1960\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.897609756097561\n",
      "Iteration:  1970\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8977317073170732\n",
      "Iteration:  1980\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.898\n",
      "Iteration:  1990\n",
      "[9 1 4 ... 9 6 2] [9 1 4 ... 9 6 2]\n",
      "0.8983170731707317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    iterations : 900 \\n    and learning rate(alpha) : 0.1\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And Finally getting the prediction scores:\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.1 , 2000)\n",
    "'''\n",
    "    iterations : 900 \n",
    "    and learning rate(alpha) : 0.1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d805151-9b0e-4f0d-adf4-b5ee82303711",
   "metadata": {},
   "source": [
    "Accuracy score : ~89% on training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c9c2dca-d628-4b4e-b319-0eb03af55db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90dc3133-f66e-4937-95e4-3514814fb98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [6]\n",
      "Label:  6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction(500, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "874449fe-cd88-4fea-8be9-1f266399b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 8 7 8 8 4 4 7 6 1 6 0 3 2 2 5 4 5 7 4 5 2 4 7 4 0 0 0 9 4 0 0 4 8 6 7 0\n",
      " 6 7 2 9 0 9 2 1 4 2 5 7 6 1 5 0 1 1 0 3 3 5 8 3 5 2 6 4 6 1 7 0 5 1 5 0 2\n",
      " 0 8 0 8 2 0 8 2 0 1 2 4 5 9 7 0 8 1 5 9 7 7 8 8 9 5 6 0 3 5 1 9 3 6 7 1 2\n",
      " 1 7 3 1 1 9 1 7 2 7 8 2 3 7 9 4 0 6 2 4 4 7 3 8 4 9 9 1 3 6 5 0 9 5 3 9 0\n",
      " 2 8 3 8 1 6 1 3 2 4 3 0 7 4 5 3 9 4 4 1 0 3 1 3 3 4 4 7 3 1 9 1 1 6 6 7 5\n",
      " 3 8 5 0 1 4 6 2 2 3 3 6 1 5 1 5 9 6 9 2 2 2 8 5 3 6 8 0 5 1 2 0 6 0 8 4 7\n",
      " 1 6 3 8 3 7 6 5 9 4 7 6 0 2 9 4 5 6 1 7 1 2 2 8 8 6 3 1 1 4 4 5 0 8 1 5 1\n",
      " 5 1 7 2 0 0 8 1 2 4 2 4 2 0 5 0 2 1 1 7 9 5 5 6 1 8 3 1 8 7 4 3 4 0 2 5 4\n",
      " 0 5 3 7 4 8 1 0 7 7 6 7 0 5 4 7 8 1 0 7 8 6 4 5 2 2 7 8 7 9 1 6 4 7 6 7 8\n",
      " 0 6 3 0 3 6 1 2 4 5 9 8 8 5 4 5 1 3 0 2 6 7 8 6 9 1 5 5 2 3 3 6 5 3 2 7 0\n",
      " 6 6 0 1 3 8 9 7 8 5 8 4 7 5 0 1 1 3 4 1 1 6 9 7 6 7 0 1 7 6 3 8 9 7 1 5 7\n",
      " 3 7 1 5 4 8 4 5 1 8 3 8 7 9 9 5 1 4 3 0 3 4 2 5 5 2 4 2 2 5 7 1 8 3 3 5 2\n",
      " 7 1 8 4 2 2 3 1 3 5 7 1 4 4 1 3 7 4 4 7 6 4 6 2 4 1 2 8 9 6 5 5 4 3 2 4 4\n",
      " 0 3 6 4 7 9 3 8 4 7 7 7 5 7 0 0 2 5 7 6 8 8 3 7 9 1 8 8 7 2 5 2 5 9 3 3 0\n",
      " 7 7 6 1 8 2 9 0 5 9 4 9 8 0 7 4 1 6 0 6 8 9 6 2 0 1 3 9 4 8 9 7 8 0 8 2 3\n",
      " 6 7 3 0 9 8 9 1 6 6 3 4 1 3 9 8 1 2 6 1 7 8 6 7 6 9 7 9 9 3 5 9 9 7 1 9 7\n",
      " 6 8 4 1 1 3 1 8 8 2 1 8 2 4 6 9 2 5 6 6 7 5 8 9 0 6 5 1 6 7 8 8 4 5 1 5 1\n",
      " 7 0 3 0 3 7 2 1 9 5 2 7 0 6 4 9 2 9 6 5 2 0 9 3 0 7 0 1 7 4 7 3 3 3 1 9 3\n",
      " 3 4 9 2 3 7 5 3 3 6 9 5 8 5 6 7 0 1 6 3 0 4 7 2 8 4 1 0 3 1 2 9 9 4 2 5 0\n",
      " 5 0 7 1 0 6 4 1 8 3 6 4 2 8 6 9 1 4 8 3 4 8 2 7 6 5 2 3 9 2 2 4 9 3 2 1 1\n",
      " 1 9 1 9 3 9 6 4 9 9 3 2 3 3 4 2 2 8 1 5 7 7 5 7 7 1 5 8 9 0 0 1 8 4 4 1 0\n",
      " 5 2 8 7 2 6 3 4 8 1 7 8 9 6 4 5 0 7 9 1 2 8 1 1 5 8 7 7 9 7 7 8 4 3 5 4 7\n",
      " 8 3 9 7 2 2 0 6 6 4 4 6 9 6 4 1 2 8 5 6 5 3 5 5 0 3 1 4 0 9 4 4 5 7 6 1 4\n",
      " 0 8 0 5 4 7 6 8 2 7 9 9 2 0 3 7 1 2 6 0 3 1 3 5 4 8 8 8 7 1 9 5 1 6 3 3 4\n",
      " 4 0 9 0 1 4 8 1 1 2 9 7 7 9 3 9 0 2 7 1 7 5 6 9 5 6 9 7 9 0 3 0 8 9 8 3 4\n",
      " 6 2 0 9 0 5 4 6 7 4 0 4 9 7 7 4 0 4 1 3 5 1 1 1 2 6 1 6 9 3 5 2 9 9 8 9 1\n",
      " 2 7 0 3 7 5 0 1 6 5 0 7 9 6 6 0 9 4 7 6 9 2 7 1 0 6 9 9 3 8 8 3 2 1 1 1 3\n",
      " 1] [5 8 7 8 8 0 4 7 6 1 6 0 3 2 2 5 4 5 7 4 5 2 4 7 7 0 0 0 9 9 0 0 4 8 6 7 6\n",
      " 6 7 2 9 0 7 3 1 9 2 5 7 6 1 8 0 1 1 0 3 3 5 8 3 5 2 6 4 6 1 9 0 5 1 5 0 2\n",
      " 0 8 0 8 2 0 8 2 0 1 2 4 5 9 7 0 8 1 5 9 7 7 8 8 9 5 6 0 3 5 1 8 3 6 7 3 6\n",
      " 1 7 3 1 1 9 1 7 2 7 8 2 3 7 9 4 0 6 2 4 9 7 3 8 4 9 9 1 3 6 0 0 9 5 3 9 0\n",
      " 6 2 3 8 1 6 1 3 2 4 5 0 7 9 9 3 9 4 4 1 0 3 1 3 3 4 4 7 3 1 9 1 1 6 6 7 5\n",
      " 3 8 5 0 1 4 6 2 2 3 5 6 2 5 1 5 9 6 9 2 4 2 8 5 3 6 8 0 8 1 2 0 6 0 8 4 7\n",
      " 1 6 3 8 3 7 6 5 9 4 7 6 0 2 9 4 5 6 1 7 1 2 2 8 9 6 3 1 1 9 7 5 0 8 1 5 1\n",
      " 5 1 7 2 0 0 8 1 2 4 2 4 2 0 5 0 2 1 1 7 9 5 5 6 1 8 3 1 8 7 4 3 4 0 2 5 4\n",
      " 0 5 9 7 6 8 1 0 7 7 6 7 0 5 4 7 8 1 0 7 8 6 4 5 2 2 9 8 7 9 1 6 4 7 6 7 8\n",
      " 0 6 3 0 3 6 1 7 4 5 0 8 8 5 4 5 1 3 0 2 6 7 8 2 9 1 5 5 2 3 3 6 5 3 2 7 0\n",
      " 6 6 0 1 3 8 9 7 8 5 8 4 7 5 0 1 1 3 4 1 1 6 9 7 6 7 0 1 7 6 3 8 9 7 1 5 7\n",
      " 8 7 1 5 9 8 4 5 1 8 8 8 7 9 9 5 1 4 3 0 9 4 7 5 3 2 4 2 2 5 7 1 8 2 3 5 4\n",
      " 7 1 8 4 2 2 3 1 3 5 7 1 2 4 7 3 7 4 4 7 6 4 6 2 4 1 2 8 9 6 8 5 4 3 2 4 4\n",
      " 0 3 6 4 7 9 3 8 4 7 7 7 5 7 0 0 6 5 7 6 8 5 3 7 9 1 8 8 7 2 5 2 5 7 3 5 0\n",
      " 7 7 6 1 8 2 3 0 5 4 4 9 8 0 7 4 1 6 0 6 8 9 6 2 0 1 3 9 4 8 9 7 8 0 8 2 7\n",
      " 6 7 3 0 9 2 9 1 6 6 3 4 1 3 4 8 1 2 6 1 9 8 6 7 6 9 7 9 9 3 5 9 9 7 1 9 7\n",
      " 6 8 4 1 1 3 1 8 1 2 1 8 2 4 6 9 2 3 6 6 7 5 8 9 0 6 5 1 6 7 8 3 4 3 1 5 1\n",
      " 7 0 3 8 3 7 2 1 9 5 2 7 0 6 4 9 3 9 6 5 2 0 9 3 0 7 0 1 7 4 7 8 3 3 1 9 3\n",
      " 3 4 9 2 3 7 5 5 3 6 9 5 8 5 6 7 0 1 6 3 0 4 7 2 2 9 1 0 3 1 2 9 9 4 2 8 5\n",
      " 5 0 7 1 0 6 4 1 8 3 6 4 2 8 2 9 1 9 8 3 4 8 3 7 6 5 2 3 9 2 2 4 9 3 2 1 1\n",
      " 1 9 1 9 3 9 6 4 9 9 3 2 5 3 4 7 2 8 2 5 7 7 5 7 7 1 5 8 9 0 0 1 8 4 4 1 0\n",
      " 5 2 8 7 2 6 3 4 8 1 7 8 7 6 4 5 0 7 9 1 2 8 1 1 3 8 7 7 9 7 7 8 4 3 5 4 7\n",
      " 8 9 4 7 2 2 0 6 6 4 5 6 9 6 4 1 2 8 5 6 3 3 5 5 0 3 1 4 0 9 4 9 5 7 6 1 4\n",
      " 0 8 0 5 4 7 6 8 2 9 9 4 2 0 3 7 1 2 6 0 3 1 3 5 4 8 8 8 7 1 9 5 1 6 3 3 4\n",
      " 4 6 9 0 1 4 8 1 1 2 9 7 7 9 3 9 0 2 7 1 7 8 6 9 5 6 9 7 9 0 3 0 8 9 8 3 4\n",
      " 6 2 0 9 0 5 8 6 7 4 0 4 9 8 7 7 0 9 1 3 5 1 1 1 0 6 8 6 4 3 5 2 9 9 4 4 1\n",
      " 2 7 0 3 7 5 0 1 6 9 0 7 9 6 4 2 9 4 7 5 9 2 7 1 0 8 4 9 3 1 8 3 2 1 1 1 3\n",
      " 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n",
    "get_accuracy(dev_predictions, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44332b-efa7-4fc1-940f-5c514e4820a7",
   "metadata": {},
   "source": [
    "~Cross_Validation test accuracy of 90% 🏆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c7620-b795-482b-aafd-d0739a0f2733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
